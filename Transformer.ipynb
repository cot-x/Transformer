{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from comet_ml import Experiment\n",
    "#experiment = Experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Ccpy4OkFMEM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import argparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFile\n",
    "from pickle import load, dump\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#import collections\n",
    "import pandas as pd\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "from torch import einsum\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextData:\n",
    "    texts1 = []\n",
    "    texts2 = []\n",
    "    \n",
    "    def __init__(self, csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        eng = df.iloc[:, 1]\n",
    "        jp = df.iloc[:, 2]\n",
    "\n",
    "        print('Load text1 from datasets.')\n",
    "        tokenizer = Tokenizer()\n",
    "        self.texts1 = [list(tokenizer.tokenize(t.strip(), wakati=True)) for t in tqdm(jp)]\n",
    "        \n",
    "        print('Load text2 from datasets.')\n",
    "        self.texts2 = [s.split() for s in\n",
    "                       [' '.join(re.split('([' + string.punctuation + '])', t.strip())) for t in tqdm(eng)]]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return list(self.texts1[index]), list(self.texts2[index])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts1)\n",
    "    \n",
    "    def tolist(self):\n",
    "        return sum(self.texts1, []) + sum(self.texts2, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    @staticmethod\n",
    "    def make_vocab(text_data: TextData, vocab_size=None):\n",
    "        print('Generate word-ids.')\n",
    "        word2id = {}\n",
    "        word2id['<pad>'] = 0\n",
    "        word2id['<unk>'] = 1\n",
    "        word2id['<s>'] = 2\n",
    "        \n",
    "        #wc = collections.Counter(text_data.tolist())\n",
    "        #for i, (w, _) in enumerate(wc.most_common(vocab_size), 3):\n",
    "        #    word2id[w] = i\n",
    "        \n",
    "        id2word = {v: k for k, v in word2id.items()}\n",
    "        \n",
    "        for words1, words2 in tqdm(text_data):\n",
    "            for word in (words1 + words2):\n",
    "                if word not in word2id:\n",
    "                    id = len(word2id)\n",
    "                    word2id[word] = id\n",
    "                    id2word[id] = word\n",
    "        \n",
    "        return word2id, id2word\n",
    "    \n",
    "    def to_string(self, data):\n",
    "        text = ''\n",
    "        for d in data:\n",
    "            try:\n",
    "                text += self.id2word[int(d)] + ' '\n",
    "            except:\n",
    "                text += '<unk>'\n",
    "        return text\n",
    "    \n",
    "    def to_tokens(self, data):\n",
    "        tokens = []\n",
    "        for d in data:\n",
    "            try:\n",
    "                tokens += [self.word2id[d]]\n",
    "            except:\n",
    "                tokens += [self.word2id['<unk>']]\n",
    "        return tokens\n",
    "    \n",
    "    def __init__(self, csv_path, sentence_size, vocab_size=None):\n",
    "        self.sentence_size = sentence_size\n",
    "        \n",
    "        if os.path.exists('textdata.dat'):\n",
    "            with open(os.path.join('.', 'textdata.dat'), 'rb') as f:\n",
    "                self.text_data = load(f)\n",
    "                print('Loaded textdata.dat.')\n",
    "        else:\n",
    "            self.text_data = TextData(csv_path)\n",
    "            with open(os.path.join('.', 'textdata.dat'), 'wb') as f:\n",
    "                dump(self.text_data, f)\n",
    "                print('Saved textdata.dat.')\n",
    "        \n",
    "        if os.path.exists('word2id.dat') and os.path.exists('id2word.dat'):\n",
    "            with open(os.path.join('.', 'word2id.dat'), 'rb') as f:\n",
    "                self.word2id = load(f)\n",
    "                print('Loaded word2id.dat.')\n",
    "            with open(os.path.join('.', 'id2word.dat'), 'rb') as f:\n",
    "                self.id2word = load(f)\n",
    "                print('Loaded id2word.dat.')\n",
    "        else:\n",
    "            self.word2id, self.id2word = TextDataset.make_vocab(self.text_data, vocab_size)\n",
    "            with open(os.path.join('.', 'word2id.dat'), 'wb') as f:\n",
    "                dump(self.word2id, f)\n",
    "                print('Saved word2id.dat.')\n",
    "            with open(os.path.join('.', 'id2word.dat'), 'wb') as f:\n",
    "                dump(self.id2word, f)\n",
    "                print('Saved id2word.dat.')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text1, text2 = self.text_data[index]\n",
    "        tokens1 = self.to_tokens(text1)\n",
    "        tokens2 = self.to_tokens(text2)\n",
    "        \n",
    "        tokens1 = [self.word2id['<s>']] + tokens1\n",
    "        tokens2 = [self.word2id['<s>']] + tokens2\n",
    "        \n",
    "        tokens1 = tokens1[:self.sentence_size]\n",
    "        tokens2 = tokens2[:self.sentence_size]\n",
    "        \n",
    "        tokens1.extend([self.word2id['<pad>'] for _ in range(self.sentence_size - len(tokens1))])\n",
    "        tokens2.extend([self.word2id['<pad>'] for _ in range(self.sentence_size - len(tokens2))])\n",
    "        \n",
    "        tokens1 = torch.LongTensor(tokens1)\n",
    "        tokens2 = torch.LongTensor(tokens2)\n",
    "        \n",
    "        return tokens1, tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(nn.Module):\n",
    "    @staticmethod\n",
    "    def mish(x):\n",
    "        return x * torch.tanh(F.softplus(x))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return Mish.mish(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, sentence_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.pe = torch.Tensor(sentence_size, vocab_size)\n",
    "        for pos in range(sentence_size):\n",
    "            for i in range(0, vocab_size, 2):\n",
    "                self.pe[pos, i] = math.sin(pos / (10000**((2*i)/vocab_size)))\n",
    "                self.pe[pos, i+1] = math.cos(pos / (10000**((2*(i+1))/vocab_size)))\n",
    "        self.pe = self.pe.unsqueeze(0)\n",
    "        self.pe.requires_grad = False\n",
    "    \n",
    "    def to(self, device):\n",
    "        self.pe = self.pe.to(device)\n",
    "        return super().to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return math.sqrt(self.vocab_size) * x + self.pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, sentence_size, dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.word_embedding = nn.Embedding(vocab_size, dim)\n",
    "        #self.position_embedding = nn.Embedding(sentence_size, dim)\n",
    "        self.position_embedding = PositionalEncoder(dim, sentence_size)\n",
    "        self.LayerNorm = nn.LayerNorm(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        word_embedding = self.word_embedding(input_ids)\n",
    "        \n",
    "        #position_ids = torch.arange(input_ids.size(1), dtype=torch.long, device=input_ids.device)\n",
    "        #position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        #position_embedding = self.position_embedding(position_ids)\n",
    "        \n",
    "        #embedding = word_embedding + position_embedding\n",
    "        embedding = self.position_embedding(word_embedding)\n",
    "        embedding = self.LayerNorm(embedding)\n",
    "        embedding = self.dropout(embedding)\n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, cross=False, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        \n",
    "        inner_dim = dim_head * heads\n",
    "        self.to_query = nn.Linear(dim, inner_dim, bias = False)\n",
    "        if not cross:\n",
    "            self.to_key = nn.Linear(dim, inner_dim, bias = False)\n",
    "            self.to_value = nn.Linear(dim, inner_dim, bias = False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if not (heads == 1 and dim_head == dim) else nn.Identity()\n",
    "        \n",
    "    def forward(self, x, kv=None, mask=None, return_attention=False):\n",
    "        query = self.to_query(x)\n",
    "        if kv != None:\n",
    "            key = value = kv\n",
    "        else:\n",
    "            key = self.to_key(x)\n",
    "            value = self.to_value(x)\n",
    "        \n",
    "        query = rearrange(query, 'b n (h d) -> b h n d', h = self.heads)\n",
    "        key = rearrange(key, 'b n (h d) -> b h n d', h = self.heads)\n",
    "        value = rearrange(value, 'b n (h d) -> b h n d', h = self.heads)\n",
    "        \n",
    "        attention_score = torch.matmul(query, key.transpose(-1, -2)) * self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            attention_score = attention_score.masked_fill(mask==0, float('-inf'))\n",
    "        \n",
    "        attention_prob = F.softmax(attention_score, dim=-1)\n",
    "        attention_prob = self.dropout(attention_prob)\n",
    "        \n",
    "        context = torch.matmul(attention_prob, value)\n",
    "        context = rearrange(context, 'b h n d -> b n (h d)')\n",
    "        \n",
    "        if return_attention:\n",
    "            return self.to_out(context), attention_prob\n",
    "        else:\n",
    "            return self.to_out(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            Mish(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim, mlp_dim, depth=4, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "            ]))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for attention, feedforward in self.layers:\n",
    "            x = attention(x) + x\n",
    "            x = feedforward(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dim, mlp_dim, vocab_size, depth=4, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                PreNorm(dim, Attention(dim, cross=True, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "            ]))\n",
    "    \n",
    "    def forward(self, x, kv):\n",
    "        tri_mask = torch.triu(torch.ones(x.size(-2), x.size(-2))).transpose(1, 0).to(x.device)\n",
    "        for attention, cross_attention, feedforward in self.layers:\n",
    "            x = attention(x, mask=tri_mask) + x\n",
    "            x = cross_attention(x, kv=kv) + x\n",
    "            x = feedforward(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, sentence_size, dim=512, mlp_dim=1024, dropout=0., emb_dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = Embedding(vocab_size, sentence_size, dim, emb_dropout)\n",
    "        self.encoder = Encoder(dim, mlp_dim, dropout=dropout)\n",
    "        self.decoder = Decoder(dim, mlp_dim, vocab_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(dim, vocab_size) # トークン出力分布\n",
    "    \n",
    "    def to(self, *args, **kwargs):\n",
    "        self.embedding.position_embedding.to(args[0])\n",
    "        return super().to(*args, **kwargs)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        x = self.embedding(x)\n",
    "        y = self.embedding(y)\n",
    "        enc = self.encoder(x)\n",
    "        dec = self.decoder(y, enc)\n",
    "        out = self.linear(dec)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self, args):\n",
    "        use_cuda = torch.cuda.is_available() if not args.cpu else False\n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(f'Use Device: {self.device}')\n",
    "        \n",
    "        self.args = args\n",
    "        \n",
    "        self.dataset = TextDataset(self.args.csv_path, self.args.sentence_size)\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=self.args.batch_size, shuffle=True, drop_last=True)\n",
    "        self.vocab_size = len(self.dataset.word2id)\n",
    "        \n",
    "        self.transformer = Transformer(self.vocab_size, self.args.sentence_size,\n",
    "                                       dropout=self.args.dropout_prob).to(self.device)\n",
    "        self.transformer.apply(self.weights_init)\n",
    "        self.optimizer = optim.Adam(self.transformer.parameters(), lr=self.args.lr)\n",
    "        \n",
    "        self.epoch = 0\n",
    "    \n",
    "    def weights_init(self, module):\n",
    "        if type(module) == nn.Linear or type(module) == nn.Conv2d or type(module) == nn.ConvTranspose2d:\n",
    "            nn.init.kaiming_normal_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.fill_(0)\n",
    "    \n",
    "    def save_state(self, epoch):\n",
    "        self.transformer.cpu()\n",
    "        torch.save(self.transformer.state_dict(), os.path.join(self.args.weight_dir, f'weight.{epoch}.pth'))\n",
    "        self.transformer.to(self.device)\n",
    "        \n",
    "    def load_state(self):\n",
    "        if os.path.exists('weight.pth'):\n",
    "            self.transformer.load_state_dict(torch.load('weight.pth', map_location=self.device))\n",
    "            print('Loaded network state.')\n",
    "    \n",
    "    def save_resume(self):\n",
    "        with open(os.path.join('.', f'resume.pkl'), 'wb') as f:\n",
    "            dump(self, f)\n",
    "    \n",
    "    def load_resume(self):\n",
    "        if os.path.exists('resume.pkl'):\n",
    "            with open(os.path.join('.', 'resume.pkl'), 'rb') as f:\n",
    "                print('Load resume.')\n",
    "                return load(f)\n",
    "        else:\n",
    "            return self\n",
    "    \n",
    "    def trainTransformer(self, epoch, iters, max_iters, text1, text2):\n",
    "        softmax_crossentropy = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Compute loss.\n",
    "        text_evals = self.transformer(text1, text2)\n",
    "        text_evals_reshape = text_evals.reshape(-1, text_evals.size(-1))\n",
    "        \n",
    "        pad = torch.LongTensor([self.dataset.word2id['<pad>']]).unsqueeze(0).repeat(text2.size(0), 1).to(self.device)\n",
    "        text_study = torch.cat((text2[:, 1:], pad), dim=1)\n",
    "        text_study_reshape = text_study.reshape(-1)\n",
    "        \n",
    "        loss = softmax_crossentropy(text_evals_reshape, text_study_reshape)\n",
    "        \n",
    "        # Backward and optimize.\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Logging.\n",
    "        losses = {}\n",
    "        losses['Loss'] = loss.item()\n",
    "        \n",
    "        # Save.\n",
    "        if iters == max_iters:\n",
    "            text_prob = torch.softmax(text_evals[0], dim=-1)\n",
    "            text = text_prob.argmax(dim=-1)\n",
    "            print(self.dataset.to_string(text1[0]))\n",
    "            print('================>>>>')\n",
    "            print(self.dataset.to_string(text))\n",
    "            self.save_state(epoch)\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def train(self):\n",
    "        self.transformer.train()\n",
    "        \n",
    "        hyper_params = {}\n",
    "        hyper_params['CSV Path'] = args.csv_path\n",
    "        hyper_params['Weight Dir'] = args.weight_dir\n",
    "        hyper_params['Sentence Size'] = args.sentence_size\n",
    "        hyper_params['Dropout Prob.'] = args.dropout_prob\n",
    "        hyper_params['Learning Rate'] = args.lr\n",
    "        hyper_params['Batch Size'] = args.batch_size\n",
    "        hyper_params['Num Train'] = args.num_train\n",
    "\n",
    "        for key in hyper_params.keys():\n",
    "            print(f'{key}: {hyper_params[key]}')\n",
    "        #experiment.log_parameters(hyper_params)\n",
    "        \n",
    "        while self.args.num_train > self.epoch:\n",
    "            self.epoch += 1\n",
    "            epoch_loss = 0.0\n",
    "            \n",
    "            max_iters = len(self.dataloader)\n",
    "            for iters, (text1, text2) in enumerate(tqdm(self.dataloader)):\n",
    "                iters += 1\n",
    "                \n",
    "                text1 = text1.to(self.device)\n",
    "                text2 = text2.to(self.device)\n",
    "                \n",
    "                losses = self.trainTransformer(self.epoch, iters, max_iters, text1, text2)\n",
    "                \n",
    "                epoch_loss += losses['Loss']\n",
    "                #experiment.log_metrics(losses)\n",
    "            \n",
    "            print(f'Epoch[{self.epoch}] Loss({epoch_loss})')\n",
    "                    \n",
    "            if not self.args.noresume:\n",
    "                self.save_resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    solver = Solver(args)\n",
    "    solver.load_state()\n",
    "    \n",
    "    if not args.noresume:\n",
    "        solver = solver.load_resume()\n",
    "        solver.args = args\n",
    "    \n",
    "    solver.train()\n",
    "    \n",
    "    #experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--csv_path', type=str, default='')\n",
    "    parser.add_argument('--weight_dir', type=str, default='weights')\n",
    "    parser.add_argument('--sentence_size', type=int, default=128)\n",
    "    parser.add_argument('--dropout_prob', type=float, default=0.1)\n",
    "    parser.add_argument('--lr', type=float, default=0.0001)\n",
    "    parser.add_argument('--batch_size', type=int, default=8)\n",
    "    parser.add_argument('--num_train', type=int, default=10)\n",
    "    parser.add_argument('--cpu', action='store_true')\n",
    "    parser.add_argument('--noresume', action='store_true')\n",
    "    \n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    if not os.path.exists(args.weight_dir):\n",
    "        os.mkdir(args.weight_dir)\n",
    "    \n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CycleGAN.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 534269,
     "sourceId": 1242223,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
